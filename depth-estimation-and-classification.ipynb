{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10490741,"sourceType":"datasetVersion","datasetId":6495491},{"sourceId":10490887,"sourceType":"datasetVersion","datasetId":6495605}],"dockerImageVersionId":30840,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from transformers import pipeline\nfrom PIL import Image\nfrom accelerate.test_utils.testing import get_backend\nimport matplotlib.pyplot as plt\n\n# Set up the backend device\ndevice, _, _ = get_backend()\n\n# Load the depth estimation model\ncheckpoint = \"depth-anything/Depth-Anything-V2-base-hf\"\npipe = pipeline(\"depth-estimation\", model=checkpoint, device=device)\n\n# Define input and output paths\ninput_path = \"/kaggle/input/sampleapple/splitted-DJI_20240901104507_0236_D_clahe_grid_3_5.jpg\"\noutput_path = \"/kaggle/working/depth_map.png\"\n\n# Load the image\nimage = Image.open(input_path).convert(\"RGB\")\n\n# Perform depth estimation\npredictions = pipe(image)\ndepth_map = predictions[\"depth\"]\n\n# Save the depth map directly if it's a PIL image\nif isinstance(depth_map, Image.Image):\n    depth_map.save(output_path)\n    print(f\"Depth map saved as image at: {output_path}\")\nelse:\n    # If depth_map is not an image, assume it's a tensor or numpy array\n    import numpy as np\n    depth_array = depth_map.squeeze()  # Ensure it's a 2D array\n    plt.imshow(depth_array, cmap=\"viridis\")\n    plt.axis(\"off\")\n    plt.savefig(output_path, bbox_inches='tight', pad_inches=0)\n    plt.close()\n    print(f\"Depth map saved as visualization at: {output_path}\")\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-01-29T01:26:46.906626Z","iopub.execute_input":"2025-01-29T01:26:46.906876Z","iopub.status.idle":"2025-01-29T01:27:19.758673Z","shell.execute_reply.started":"2025-01-29T01:26:46.906856Z","shell.execute_reply":"2025-01-29T01:27:19.757866Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/953 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d6889f9dd2f243259b28b3472852dc02"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/390M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"25f1184b77d74da49c16a1cf834a6163"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/775 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9f00100fc9bf4ccbafaa533613d0f706"}},"metadata":{}},{"name":"stderr","text":"Device set to use cuda\n","output_type":"stream"},{"name":"stdout","text":"Depth map saved as image at: /kaggle/working/depth_map.png\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"from transformers import pipeline\nfrom PIL import Image, ImageDraw\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom accelerate.test_utils.testing import get_backend\n\n# Set up the backend device\ndevice, _, _ = get_backend()\n\n# Load the depth estimation model\ndepth_checkpoint = \"depth-anything/Depth-Anything-V2-base-hf\"\ndepth_pipe = pipeline(\"depth-estimation\", model=depth_checkpoint, device=device)\n\n# Load the object detection model\nobject_detection_pipe = pipeline(\"object-detection\", model=\"facebook/detr-resnet-50\", device=device)\n\n# Define input and output paths\ninput_path = \"/kaggle/input/sampleapple/splitted-DJI_20240901104507_0236_D_clahe_grid_3_5.jpg\"\noutput_path = \"/kaggle/working/depth_map_with_apple_borders.png\"\n\n# Load the image\nimage = Image.open(input_path).convert(\"RGB\")\n\n# Perform depth estimation\ndepth_predictions = depth_pipe(image)\ndepth_map = depth_predictions[\"depth\"]\n\n# Perform object detection\ndetection_results = object_detection_pipe(image)\n\n# Highlight detected apples on the depth map with red borders only\nif isinstance(depth_map, Image.Image):\n    depth_map = depth_map.convert(\"RGBA\")  # Ensure the depth map is in RGBA mode for overlay\nelse:\n    raise ValueError(\"Depth map should be a PIL Image for this implementation.\")\n\ndraw = ImageDraw.Draw(depth_map)\n\n# Filter detection results for \"apple\"\nfor detection in detection_results:\n    if \"apple\" in detection[\"label\"].lower() and detection[\"score\"] > 0.5:\n        bbox = detection[\"box\"]\n        draw.rectangle(\n            [bbox[\"xmin\"], bbox[\"ymin\"], bbox[\"xmax\"], bbox[\"ymax\"]],\n            outline=\"red\",  # Red border for apples\n            width=5,        # Border width\n        )\n\n# Save the resulting image\ndepth_map.save(output_path)\nprint(f\"Depth map with apple borders saved at: {output_path}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-29T01:27:19.759570Z","iopub.execute_input":"2025-01-29T01:27:19.760254Z","iopub.status.idle":"2025-01-29T01:27:25.876640Z","shell.execute_reply.started":"2025-01-29T01:27:19.760209Z","shell.execute_reply":"2025-01-29T01:27:25.875692Z"}},"outputs":[{"name":"stderr","text":"Device set to use cuda\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/4.59k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f7ee9a2dc9204eedaec5876177332480"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/167M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"05a8f82f251340b0a99dafd8b80ae7df"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/102M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d954c8c833f3497981ef9b2261aadabc"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at facebook/detr-resnet-50 were not used when initializing DetrForObjectDetection: ['model.backbone.conv_encoder.model.layer1.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer2.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer3.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer4.0.downsample.1.num_batches_tracked']\n- This IS expected if you are initializing DetrForObjectDetection from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DetrForObjectDetection from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/290 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"154edbecd16e448782ab6f6f4e4216f3"}},"metadata":{}},{"name":"stderr","text":"Device set to use cuda\n","output_type":"stream"},{"name":"stdout","text":"Depth map with apple borders saved at: /kaggle/working/depth_map_with_apple_borders.png\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import os\nfrom transformers import pipeline\nfrom PIL import Image, ImageDraw\nfrom accelerate.test_utils.testing import get_backend\n\n# Set up the backend device\ndevice, _, _ = get_backend()\n\n# Load the depth estimation model\ndepth_checkpoint = \"depth-anything/Depth-Anything-V2-base-hf\"\ndepth_pipe = pipeline(\"depth-estimation\", model=depth_checkpoint, device=device)\n\n# Load the object detection model\nobject_detection_pipe = pipeline(\"object-detection\", model=\"facebook/detr-resnet-50\", device=device)\n\n# Define input and output paths\ninput_folder = \"/kaggle/input/mutlitest\"\noutput_folder = \"/kaggle/working/multiple_outputs\"\nos.makedirs(output_folder, exist_ok=True)  # Create the output folder if it doesn't exist\n\n# Process each image in the input folder\nfor filename in os.listdir(input_folder):\n    if filename.endswith((\".jpg\", \".jpeg\", \".png\")):  # Process only image files\n        input_path = os.path.join(input_folder, filename)\n        output_path = os.path.join(output_folder, f\"processed_{filename}\")\n        \n        # Load the image\n        image = Image.open(input_path).convert(\"RGB\")\n        \n        # Perform depth estimation\n        depth_predictions = depth_pipe(image)\n        depth_map = depth_predictions[\"depth\"]\n\n        # Perform object detection\n        detection_results = object_detection_pipe(image)\n\n        # Highlight detected apples on the depth map with red borders only\n        if isinstance(depth_map, Image.Image):\n            depth_map = depth_map.convert(\"RGBA\")  # Ensure the depth map is in RGBA mode for overlay\n        else:\n            raise ValueError(\"Depth map should be a PIL Image for this implementation.\")\n\n        draw = ImageDraw.Draw(depth_map)\n\n        # Filter detection results for \"apple\"\n        for detection in detection_results:\n            if \"apple\" in detection[\"label\"].lower() and detection[\"score\"] > 0.5:\n                bbox = detection[\"box\"]\n                draw.rectangle(\n                    [bbox[\"xmin\"], bbox[\"ymin\"], bbox[\"xmax\"], bbox[\"ymax\"]],\n                    outline=\"red\",  # Red border for apples\n                    width=5,        # Border width\n                )\n\n        # Convert to RGB mode if saving as JPEG\n        if output_path.endswith((\".jpg\", \".jpeg\")):\n            depth_map = depth_map.convert(\"RGB\")\n        \n        # Save the resulting image\n        depth_map.save(output_path)\n        print(f\"Processed {filename} saved to {output_path}\")\n\nprint(f\"All images processed. Results saved in: {output_folder}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-29T01:27:25.877619Z","iopub.execute_input":"2025-01-29T01:27:25.877928Z","iopub.status.idle":"2025-01-29T01:27:28.644403Z","shell.execute_reply.started":"2025-01-29T01:27:25.877904Z","shell.execute_reply":"2025-01-29T01:27:28.643452Z"}},"outputs":[{"name":"stderr","text":"Device set to use cuda\nSome weights of the model checkpoint at facebook/detr-resnet-50 were not used when initializing DetrForObjectDetection: ['model.backbone.conv_encoder.model.layer1.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer2.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer3.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer4.0.downsample.1.num_batches_tracked']\n- This IS expected if you are initializing DetrForObjectDetection from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DetrForObjectDetection from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nDevice set to use cuda\n","output_type":"stream"},{"name":"stdout","text":"Processed AppleTest4.png saved to /kaggle/working/multiple_outputs/processed_AppleTest4.png\nProcessed AppleTest3.jpg saved to /kaggle/working/multiple_outputs/processed_AppleTest3.jpg\nProcessed AppleTest2.jpg saved to /kaggle/working/multiple_outputs/processed_AppleTest2.jpg\nAll images processed. Results saved in: /kaggle/working/multiple_outputs\n","output_type":"stream"}],"execution_count":3}]}